{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LAd9DsP-XrNy"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers -q\n",
        "!pip install accelerate -q\n",
        "\n",
        "# GPTQ Dependencies\n",
        "!pip install --upgrade optimum -q\n",
        "!pip install --upgrade auto-gptq -q\n",
        "\n",
        "# RAG Dependencies\n",
        "!pip install langchain -q\n",
        "!pip install -U sentence-transformers -q\n",
        "!pip install faiss-cpu -q\n",
        "\n",
        "# BERT\n",
        "!pip install bert-extractive-summarizer -q\n",
        "\n",
        "# Hosting Deps\n",
        "!pip install pyngrok -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from summarizer import Summarizer\n",
        "from pathlib import Path\n",
        "import nest_asyncio\n",
        "\n",
        "from auto_gptq import exllama_set_max_input_length\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import sys, json, re\n",
        "\n",
        "from flask import Flask, jsonify, request\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import threading\n",
        "\n",
        "PATH=\"/content/game.txt\"\n",
        "model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "revision = \"gptq-4bit-32g-actorder_True\"\n",
        "adapters_name = 'SyedTalha/Mistral-7B-Instruct-v0.2-PEFT-adapters-v2'"
      ],
      "metadata": {
        "id": "xzTsCU6bQ8ZQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W6Y_qRV2RUi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c34325a-c674-4cb7-afbc-03a6ce7520e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    padding=True,\n",
        "    padding_side = \"left\",\n",
        "    use_fast=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we use this fine-tuned model for story generation\n",
        "model_finetuned = AutoModelForCausalLM.from_pretrained(\n",
        "    adapters_name,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    revision=\"main\"\n",
        ")\n",
        "model_finetuned = exllama_set_max_input_length(model_finetuned, 8192)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOwFkVpQP78X",
        "outputId": "e7c971cd-5f01-4a75-e2a4-6f824e19ff94"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c6hSuwJPfbmL"
      },
      "outputs": [],
      "source": [
        "# we use this model to generate branching narratives from the generated story\n",
        "# or to process any other query from the user through RAG\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=False,\n",
        "    revision=revision\n",
        ")\n",
        "model = exllama_set_max_input_length(model, 8192)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1LvW6Kv3qaG6"
      },
      "outputs": [],
      "source": [
        "# Create a pipeline\n",
        "pipe = pipeline(\n",
        "    task='text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True, # creative generation by discouraging greedy decoding\n",
        "    temperature=1,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text = False  # Only return the current output instead of returning complete prompt\n",
        ")\n",
        "\n",
        "# Create a separate pipeline for story generation\n",
        "pipe_finetuned = pipeline(\n",
        "    task='text-generation',\n",
        "    model=model_finetuned,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True, # creative generation by discouraging greedy decoding\n",
        "    temperature=1,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text = False  # Only return the current output instead of returning complete prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = Summarizer()"
      ],
      "metadata": {
        "id": "sLve-3YLRNSo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XtvuPNsOuTve"
      },
      "outputs": [],
      "source": [
        "def make_vdb(path):\n",
        "  loader = TextLoader(path)\n",
        "  doc=loader.load()\n",
        "\n",
        "  # Chunk text\n",
        "  text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0)\n",
        "  chunked_documents = text_splitter.split_documents(doc)\n",
        "\n",
        "  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "  # Load chunked documents into the FAISS index\n",
        "  db = FAISS.from_documents(chunked_documents, embeddings)\n",
        "\n",
        "  # Connect query to FAISS index using a retriever\n",
        "  retriever = db.as_retriever(\n",
        "      search_type=\"similarity\",\n",
        "      search_kwargs={'k': 3}\n",
        "  )\n",
        "\n",
        "  folder_path = Path(\"/content/faiss_index\")\n",
        "\n",
        "  if folder_path.exists():\n",
        "    old_db = FAISS.load_local(\"/content/faiss_index\", embeddings,allow_dangerous_deserialization=True)\n",
        "    db.merge_from(old_db)\n",
        "    db.save_local(\"/content/faiss_index\")\n",
        "    return db\n",
        "  else:\n",
        "    db.save_local(\"/content/faiss_index\")\n",
        "    return db\n",
        "\n",
        "def make_rag_query(query, db):\n",
        "  docs = db.similarity_search(query)\n",
        "  result=docs[0].page_content\n",
        "  return result\n",
        "\n",
        "def save_to_txt(content):\n",
        "    filename=\"/content/game.txt\"\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(content)\n",
        "\n",
        "def rag_optimize(context,query):\n",
        "    print(\"context: \", context, type(context))\n",
        "    system = f\"\"\"\n",
        "    You are excellent at creating simplified statemnet about the context given to you. Use this simplified statement to answer the question [{query}]. The answer must be concise.\n",
        "    [{context}]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # one-shot prompting\n",
        "    chat = [\n",
        "      {\"role\": \"user\", \"content\": system}\n",
        "    ]\n",
        "\n",
        "    # prepare the prompt using the chat template\n",
        "    prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
        "    # run the pipeline to generate the model output\n",
        "    outputs = pipe(prompt)\n",
        "    output = outputs[0][\"generated_text\"].strip()\n",
        "    return output\n",
        "\n",
        "def summarize_scenerio(scenerio):\n",
        "  bert_summary = ''.join(bert_model(scenerio, min_length=10))\n",
        "  return bert_summary\n",
        "\n",
        "def extract_first_json(text):\n",
        "    # Define a regex pattern to match the first JSON object\n",
        "    pattern = r'{\\s*\".*?\"\\s*:\\s*{.*?}\\s*}'\n",
        "\n",
        "    # Use re.search to find the first match of the pattern in the text\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        # Extract and return the matched JSON object\n",
        "        return match.group()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# This function takes in the malformed json and creates a prompt for the model to convert it into a valid json as per the given valid json schema.\n",
        "def json_fixer(malformed_json, valid_json_schema):\n",
        "    prompt = f\"\"\"Generate valid JSON from the malformed JSON fixing missing commas, quotes and brackets.\n",
        "\n",
        "valid JSON should strictly follow the following \"json-valid\" schema:\n",
        "```json-valid\n",
        "{valid_json_schema}\n",
        "```\n",
        "\n",
        "Here is a malformed json:\n",
        "```json-malformed\n",
        "{malformed_json}\n",
        "```\n",
        "\n",
        "Here is a fixed JSON, with fixed missing commas, quotes and brackets:\n",
        "```json\"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vbX7tRYZjYt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84c6c30-64dd-4708-a8b1-0456f4e0139b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://c2ca-34-83-116-245.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"FLASK_DEBUG\"] = \"development\"\n",
        "app = Flask(__name__)\n",
        "port = 5000\n",
        "ngrok.set_auth_token(\"2D9yNuGDT1dVBQnv20D5rMFPz38_6KYfPQZssNFuUvZcng55M\")\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(public_url)\n",
        "app.config[\"BASE_URL\"] = public_url"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/generate', methods=['POST'])\n",
        "def game():\n",
        "  data = request.json\n",
        "  story = data[\"story\"]\n",
        "  rag_query = data[\"rag_query\"]\n",
        "\n",
        "  if not rag_query:\n",
        "    # prompt for story generation\n",
        "    story_user = f\"\"\"\n",
        "    You are an AI dungeon master that provides any kind of roleplaying game content.\n",
        "\n",
        "    Instructions:\n",
        "\n",
        "    - Be specific, descriptive, and creative.\n",
        "    - Avoid repetition and avoid summarization.\n",
        "    - Generally use second person (like this: 'He looks at you.'). But use third person if that's what the story seems to follow.\n",
        "    - Never decide or write for the user. If the input ends mid sentence, continue where it left off.\n",
        "    - Make sure you always give responses continuing mid sentence even if it stops partway through.\n",
        "\n",
        "    Continue the story below:\n",
        "\n",
        "    {story}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    chat = [\n",
        "        {\"role\": \"user\", \"content\": story_user}\n",
        "    ]\n",
        "\n",
        "    # run the pipeline to generate the choices and associated damage score\n",
        "    story_prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
        "\n",
        "    # run the pipeline to generate the narrative based on the story so far\n",
        "    outputs = pipe_finetuned(story_prompt)\n",
        "    narrative = outputs[0][\"generated_text\"].strip()\n",
        "\n",
        "    # used for one-shot prompting to let the model know the output structure\n",
        "    option_assistant = f\"\"\"\n",
        "      {{\n",
        "        \"option1\": {{\n",
        "          \"text\": \"This is choice 1\",\n",
        "          \"outcome\": \"This is the narrative for choice 1\",\n",
        "          \"damage\": 0\n",
        "        }},\n",
        "        \"option2\": {{\n",
        "          \"text\": \"This is choice 2\",\n",
        "          \"outcome\": \"This is the narrative for choice 2\",\n",
        "          \"damage\": 5\n",
        "        }},\n",
        "        \"option3\": {{\n",
        "          \"text\": \"This is choice 3\",\n",
        "          \"outcome\": \"This is the narrative for choice 3\",\n",
        "          \"damage\": 0\n",
        "        }}\n",
        "      }}\n",
        "      \"\"\"\n",
        "\n",
        "    # prompt for generating branching narratives and the choices along with the associated damage score based on the most recent narrative\n",
        "    option_user = f\"\"\"\n",
        "    You are an expert interactive fiction writer who specializes in crafting short and creative branching narratvies.\\\n",
        "    Create three branching narratives for the story excerpt provided below.\\\n",
        "    The generated narrative should be one-liner sentences with less than 20 words referencing the keywords from the story and highlighting the key details.\\\n",
        "    Also, present each narrative in the form of user-visible choice as well. The choices must be of a few words capturing the essence of the resulting narrative.\\\n",
        "    Additionally, associate damage score with each choice: 5 damage if selecting this choice can bring damage, otherwise 0 damage.\n",
        "    Generate only one JSON object containing the narratives and corresponding choices using the following json schema.\n",
        "\n",
        "    ```json\n",
        "    {option_assistant}\n",
        "\n",
        "    Story Excerpt:\n",
        "\n",
        "    {narrative}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    chat = [\n",
        "        {\"role\": \"user\", \"content\": option_user},\n",
        "        {\"role\": \"assistant\", \"content\": option_assistant},\n",
        "        {\"role\": \"user\", \"content\": \"```json\"},\n",
        "    ]\n",
        "\n",
        "    # run the pipeline to generate the choices and associated damage score\n",
        "    option_prompt = tokenizer.apply_chat_template(chat, tokenize=False)\n",
        "    outputs = pipe(option_prompt)\n",
        "    output = outputs[0][\"generated_text\"].strip()\n",
        "\n",
        "    try:\n",
        "      data = json.loads(extract_first_json(output))\n",
        "    except:\n",
        "      data = None\n",
        "\n",
        "    if data is None:\n",
        "      print(\"fixing JSON!!!\") # log this instead of printing\n",
        "      prompt = json_fixer(output, option_assistant)\n",
        "      outputs = pipe(prompt)\n",
        "      output = outputs[0][\"generated_text\"].strip()\n",
        "      data = json.loads(extract_first_json(output))\n",
        "\n",
        "    option1_text = data['option1']['text']\n",
        "    option1_outcome = data['option1']['outcome']\n",
        "    option1_damage = data['option1']['damage']\n",
        "\n",
        "    option2_text = data['option2']['text']\n",
        "    option2_outcome = data['option2']['outcome']\n",
        "    option2_damage = data['option2']['damage']\n",
        "\n",
        "    option3_text = data['option3']['text']\n",
        "    option3_outcome = data['option3']['outcome']\n",
        "    option3_damage = data['option3']['damage']\n",
        "\n",
        "    options = [option1_text, option2_text, option3_text]\n",
        "    outcomes = [option1_outcome, option2_outcome, option3_outcome]\n",
        "    points = [option1_damage, option2_damage, option3_damage]\n",
        "\n",
        "    image_prompt = summarize_scenerio(narrative)\n",
        "\n",
        "    # create and store vector db for the current narrative\n",
        "    save_to_txt(narrative)\n",
        "    db = make_vdb(PATH)\n",
        "\n",
        "    return jsonify({\"story\": narrative, \"options\": options, \"outcomes\": outcomes, \"points\": points, \"image_prompt\": image_prompt, \"rag_response\": None})\n",
        "\n",
        "  else:\n",
        "    # handle rag query\n",
        "    save_to_txt(\"dummy text\")\n",
        "    db = make_vdb(PATH)\n",
        "\n",
        "    result = make_rag_query(rag_query, db)\n",
        "    rag_response = rag_optimize(result, rag_query)\n",
        "\n",
        "    return jsonify({\"story\": None, \"options\": None, \"outcomes\": None, \"points\": None, \"image_prompt\": None, \"rag_response\": rag_response})"
      ],
      "metadata": {
        "id": "UX_hIW5baOzW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threading.Thread(target=app.run, kwargs={\"use_reloader\": False}).start()"
      ],
      "metadata": {
        "id": "85NNAsJ-aUrJ"
      },
      "execution_count": 11,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}